---
title: "Zero order methods. Line search. Gradient Descent. Accelerated Gradient Descent"
author: Daniil Merkulov
institute: Applied Math for Data Science. Sberuniversity.
format: 
    beamer:
        pdf-engine: pdflatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/header.tex  # Custom LaTeX commands and preamble
header-includes:
  - \newcommand{\bgimage}{../files/back7.jpeg}
---

# Optimization without gradients

## The curse of dimensionality for zero-order methods

$$
\min_{x \in \mathbb{R}^n} f(x)
$$

. . .

$$
\text{GD: } x_{k+1} = x_k - \alpha_k \nabla f(x_k) \qquad \qquad \text{Zero order GD: } x_{k+1} = x_k - \alpha_k G,
$$

where $G$ is a 2-point or multi-point estimator of the gradient.

. . .

|  | $f(x)$ - smooth | $f(x)$ - smooth and convex | $f(x)$ - smooth and strongly convex |
|:-:|:---:|:----:|:-------:|
| GD | $\|\nabla f(x_k)\|^2 \approx \mathcal{O} \left( \dfrac{1}{k} \right)$ | $f(x_k) - f^* \approx  \mathcal{O} \left( \dfrac{1}{k} \right)$ | $\|x_k - x^*\|^2 \approx \mathcal{O} \left( \left(1 - \dfrac{\mu}{L}\right)^k \right)$ |
| Zero order GD | $\|\nabla f(x_k)\|^2 \approx \mathcal{O} \left( \dfrac{n}{k} \right)$ | $f(x_k) - f^* \approx  \mathcal{O} \left( \dfrac{n}{k} \right)$ | $\|x_k - x^*\|^2 \approx \mathcal{O} \left( \left(1 - \dfrac{\mu}{n L}\right)^k \right)$ |

## Global optimization

* [\faPython Global optimization illustration](https://colab.research.google.com/github/MerkulovDaniil/sber219/blob/main/notebooks/4_01.ipynb)
* [\faPython Nevergrad illustration](https://colab.research.google.com/github/MerkulovDaniil/sber219/blob/main/notebooks/4_02.ipynb)
* [\faPython Introduction to Optuna](https://colab.research.google.com/github/MerkulovDaniil/sber219/blob/main/notebooks/4_03.ipynb)

# Line search

## Problem

Suppose, we have a problem of minimization of a function $f(x): \mathbb{R} \to \mathbb{R}$ of scalar variable:

$$
f(x) \to \min_{x \in \mathbb{R}}
$$

. . .

Sometimes, we refer to the similar problem of finding minimum on the line segment $[a,b]$:

$$
f(x) \to \min_{x \in [a,b]}
$$

. . .


:::{.callout-example}
Typical example of line search problem is selecting appropriate stepsize for gradient descent algorithm:

$$
\begin{split}
x_{k+1} = x_k - \alpha \nabla f(x_k) \\
\alpha = \text{argmin } f(x_{k+1}) 
\end{split}
$$

:::

. . .


The line search is a fundamental optimization problem that plays a crucial role in solving complex tasks. To simplify the problem, let's assume that the function, $f(x)$, is *unimodal*, meaning it has a single peak or valley.

## Unimodal function

:::{.callout-definition}
Function $f(x)$ is called **unimodal** on $[a, b]$, if there is $x_* \in [a, b]$, that $f(x_1) > f(x_2) \;\;\; \forall a \le x_1 < x_2 < x_*$ and $f(x_1) < f(x_2) \;\;\; \forall x_* < x_1 < x_2 \leq b$
:::

. . .


![Examples of unimodal functions](unimodal.pdf)

## Key property of unimodal functions

Let $f(x)$ be unimodal function on $[a, b]$. Than if $x_1 < x_2 \in [a, b]$, then:

* if $f(x_1) \leq f(x_2) \to x_* \in [a, x_2]$
* if $f(x_1) \geq f(x_2) \to x_* \in [x_1, b]$

. . .

**Proof** Let's prove the first statement. On the contrary, suppose that $f(x_1) \leq f(x_2)$, but $x^* > x_2$. Then necessarily $x_1 < x_2 < x^*$ and by the unimodality of the function $f(x)$ the inequality: $f(x_1) > f(x_2)$ must be satisfied. We have obtained a contradiction.

. . .

:::: {.columns}
::: {.column width="33%"}
![]("Unimodal lemm1.pdf")
:::
 
. . .

::: {.column width="33%"}
![]("Unimodal lemm2.pdf")
:::

. . .

::: {.column width="33%"}
![]("Unimodal lemm3.pdf")
:::

::::

## Dichotomy method


:::: {.columns}

::: {.column width="50%"}
We aim to solve the following problem:

$$
f(x) \to \min_{x \in [a,b]}
$$

We divide a segment into two equal parts and choose the one that contains the solution of the problem using the values of functions, based on the key property described above. Our goal after one iteration of the method is to halve the solution region.
:::

::: {.column width="50%"}
![Dichotomy method for unimodal function](Dichotomy1.pdf){width=88%}
:::

::::


## Dichotomy method{.noframenumbering}

:::: {.columns}

::: {.column width="50%"}
We measure the function value at the middle of the line segment
:::

::: {.column width="50%"}
![Dichotomy method for unimodal function](Dichotomy2.pdf){width=88%}
:::

::::


## Dichotomy method{.noframenumbering}

:::: {.columns}

::: {.column width="50%"}
In order to apply the key property we perform another measurement.
:::

::: {.column width="50%"}
![Dichotomy method for unimodal function](Dichotomy3.pdf){width=88%}
:::

::::

## Dichotomy method{.noframenumbering}

:::: {.columns}

::: {.column width="50%"}
We select the target line segment. And in this case we are lucky since we already halved the solution region. But that is not always the case.
:::

::: {.column width="50%"}
![Dichotomy method for unimodal function](Dichotomy4.pdf){width=88%}
:::

::::

## Dichotomy method{.noframenumbering}

:::: {.columns}

::: {.column width="50%"}
Let's consider another unimodal function. 
:::

::: {.column width="50%"}
![Dichotomy method for unimodal function](Dichotomy5.pdf){width=88%}
:::

::::

## Dichotomy method{.noframenumbering}

:::: {.columns}

::: {.column width="50%"}
Measure the middle of the line segment.
:::

::: {.column width="50%"}
![Dichotomy method for unimodal function](Dichotomy6.pdf){width=88%}
:::

::::

## Dichotomy method{.noframenumbering}

:::: {.columns}

::: {.column width="50%"}
Get another measurement.
:::

::: {.column width="50%"}
![Dichotomy method for unimodal function](Dichotomy7.pdf){width=88%}
:::

::::

## Dichotomy method{.noframenumbering}

:::: {.columns}

::: {.column width="50%"}
Select the target line segment. You can clearly see, that the obtained line segment is not the half of the initial one. It is $\frac{3}{4} (b-a)$. So to fix it we need another step of the algorithm.
:::

::: {.column width="50%"}
![Dichotomy method for unimodal function](Dichotomy8.pdf){width=88%}
:::

::::

## Dichotomy method{.noframenumbering}

:::: {.columns}

::: {.column width="50%"}
After another additional measurement, we will surely get $\frac{2}{3} \frac{3}{4}(b-a) = \frac{1}{2}(b-a)$
:::

::: {.column width="50%"}
![Dichotomy method for unimodal function](Dichotomy9.pdf){width=88%}
:::

::::

## Dichotomy method{.noframenumbering}

:::: {.columns}

::: {.column width="50%"}
To sum it up, each subsequent iteration will require at most two function value measurements.
:::

::: {.column width="50%"}
![Dichotomy method for unimodal function](Dichotomy10.pdf){width=88%}
:::

::::


## Dichotomy method. Algorithm

```python
def binary_search(f, a, b, epsilon):
    c = (a + b) / 2
    while abs(b - a) > epsilon:
        y = (a + c) / 2.0
        if f(y) <= f(c):
            b = c
            c = y
        else:
            z = (b + c) / 2.0
            if f(c) <= f(z):
                a = y
                b = z
            else:
                a = c
                c = z
    return c
```

## Dichotomy method. Bounds
The length of the line segment on $k+1$-th iteration:

$$
\Delta_{k+1} = b_{k+1} - a_{k+1} = \dfrac{1}{2^k}(b-a)
$$


. . .



For unimodal functions, this holds if we select the middle of a segment as an output of the iteration $x_{k+1}$: 

$$
|x_{k+1} - x_*| \leq \dfrac{\Delta_{k+1}}{2} \leq \dfrac{1}{2^{k+1}}(b-a) \leq (0.5)^{k+1} \cdot (b-a)
$$


. . .



Note, that at each iteration we ask oracle no more, than 2 times, so the number of function evaluations is $N = 2 \cdot k$, which implies:

$$
|x_{k+1} - x_*| \leq (0.5)^{\frac{N}{2}+1} \cdot (b-a) \leq  (0.707)^{N}  \frac{b-a}{2}
$$

. . .


By marking the right side of the last inequality for $\varepsilon$, we get the number of method iterations needed to achieve $\varepsilon$ accuracy:

$$
K = \left\lceil \log_2 \dfrac{b-a}{\varepsilon} - 1 \right\rceil
$$

## Golden selection
The idea is quite similar to the dichotomy method. There are two golden points on the line segment (left and right) and the insightful idea is, that on the next iteration one of the points will remain the golden point.

![Key idea, that allows us to decrease function evaluations](golden_search.pdf)

## Golden selection. Algorithm

```python
def golden_search(f, a, b, epsilon):
    tau = (sqrt(5) + 1) / 2
    y = a + (b - a) / tau**2
    z = a + (b - a) / tau
    while b - a > epsilon:
        if f(y) <= f(z):
            b = z
            z = y
            y = a + (b - a) / tau**2
        else:
            a = y
            y = z
            z = a + (b - a) / tau
    return (a + b) / 2
```

## Golden selection. Bounds

$$
|x_{k+1} - x_*| \leq b_{k+1} - a_{k+1} = \left( \frac{1}{\tau} \right)^{N-1} (b - a) \approx 0.618^k(b-a),
$$

where $\tau = \frac{\sqrt{5} + 1}{2}$.

* The geometric progression constant **more** than the dichotomy method - $0.618$ worse than $0.5$
* The number of function calls **is less** than for the dichotomy method - $0.707$ worse than $0.618$ - (for each iteration of the dichotomy method, except for the first one, the function is calculated no more than 2 times, and for the gold method - no more than one)

## Successive parabolic interpolation

Sampling 3 points of a function determines unique parabola. Using this information we will go directly to its minimum. Suppose, we have 3 points $x_1 < x_2 < x_3$ such that line segment $[x_1, x_3]$ contains minimum of a function $f(x)$. Then, we need to solve the following system of equations:

. . .


$$
ax_i^2 + bx_i + c = f_i = f(x_i), i = 1,2,3 
$$

Note, that this system is linear, since we need to solve it on $a,b,c$. Minimum of this parabola will be calculated as:

. . .


$$
u = -\dfrac{b}{2a} = x_2 - \dfrac{(x_2 - x_1)^2(f_2 - f_3) - (x_2 - x_3)^2(f_2 - f_1)}{2\left[ (x_2 - x_1)(f_2 - f_3) - (x_2 - x_3)(f_2 - f_1)\right]}
$$

Note, that if $f_2 < f_1, f_2 < f_3$, than $u$ will lie in $[x_1, x_3]$

## Successive parabolic interpolation. Algorithm [^2]

\scriptsize
```python
def parabola_search(f, x1, x2, x3, epsilon):
    f1, f2, f3 = f(x1), f(x2), f(x3)
    while x3 - x1 > epsilon:
        u = x2 - ((x2 - x1)**2*(f2 - f3) - (x2 - x3)**2*(f2 - f1))/(2*((x2 - x1)*(f2 - f3) - (x2 - x3)*(f2 - f1)))
        fu = f(u)

        if x2 <= u:
            if f2 <= fu:
                x1, x2, x3 = x1, x2, u
                f1, f2, f3 = f1, f2, fu
            else:
                x1, x2, x3 = x2, u, x3
                f1, f2, f3 = f2, fu, f3
        else:
            if fu <= f2:
                x1, x2, x3 = x1, u, x2
                f1, f2, f3 = f1, fu, f2
            else:
                x1, x2, x3 = u, x2, x3
                f1, f2, f3 = fu, f2, f3
    return (x1 + x3) / 2
```


[^2]: The convergence of this method is superlinear, but local, which means, that you can take profit from using this method only near some neighbour of optimum. [*Here*](https://people.math.sc.edu/kellerlv/Quadratic_Interpolation.pdf) is the proof of superlinear convergence of order $1.32$.



## Inexact line search

:::: {.columns}

::: {.column width="50%"}
Sometimes it is enough to find a solution, which will approximately solve out problem. This is very typical scenario for mentioned stepsize selection problem

$$
\begin{split}
x_{k+1} = x_k - \alpha \nabla f(x_k) \\
\alpha = \text{argmin } f(x_{k+1}) 
\end{split}
$$

. . .


Consider a scalar function $\phi(\alpha)$ at a point $x_k$: 

$$
\phi(\alpha) = f(x_k - \alpha\nabla f(x_k)), \alpha \geq 0
$$

. . .


The first-order approximation of $\phi(\alpha)$ near $\alpha = 0$ is:

$$
\phi(\alpha) \approx f(x_k) - \alpha\nabla f(x_k)^\top \nabla f(x_k)
$$
:::

::: {.column width="50%"}
![Illustration of Taylor approximation of $\phi^I_0(\alpha)$](inexact.pdf){width=88%}
:::

::::

## Inexact line search. Sufficient Decrease

:::: {.columns}

::: {.column width="50%"}
The inexact line search condition, known as the Armijo condition, states that $\alpha$ should provide sufficient decrease in the function $f$, satisfying:

$$
f(x_k - \alpha \nabla f (x_k)) \leq f(x_k) - c_1 \cdot \alpha\nabla f(x_k)^\top \nabla f(x_k)
$$

. . .


for some constant $c_1 \in (0,1)$. Note that setting $c_1 = 1$ corresponds to the first-order Taylor approximation of $\phi(\alpha)$. However, this condition can accept very small values of $\alpha$, potentially slowing down the solution process. Typically, $c_1 \approx 10^{-4}$ is used in practice.

. . .


:::{.callout-example}
If $f(x)$ represents a cost function in an optimization problem, choosing an appropriate $c_1$ value is crucial. For instance, in a machine learning model training scenario, an improper $c_1$ might lead to either very slow convergence or missing the minimum.
:::

:::

::: {.column width="50%"}
![Illustration of sufficient decrease condition with coefficient $c_1$]("sufficient decrease.pdf"){width=88%}
:::

::::

## Inexact line search. Goldstein Conditions


:::: {.columns}

::: {.column width="50%"}
Consider two linear scalar functions $\phi_1(\alpha)$ and $\phi_2(\alpha)$:

$$
\phi_1(\alpha) = f(x_k) - c_1 \alpha \|\nabla f(x_k)\|^2
$$

$$
\phi_2(\alpha) = f(x_k) - c_2 \alpha \|\nabla f(x_k)\|^2
$$

. . .


The Goldstein-Armijo conditions locate the function $\phi(\alpha)$ between $\phi_1(\alpha)$ and $\phi_2(\alpha)$. Typically, $c_1 = \rho$ and $c_2 = 1 - \rho$, with $\rho \in (0.5, 1)$.

:::

::: {.column width="50%"}
![Illustration of Goldstein conditions](Goldstein.pdf){width=88%}
:::

::::

## Inexact line search. Curvature Condition


:::: {.columns}

::: {.column width="50%"}
To avoid excessively short steps, we introduce a second criterion:

$$
-\nabla f (x_k - \alpha \nabla f(x_k))^\top \nabla f(x_k) \geq c_2 \nabla f(x_k)^\top(- \nabla f(x_k))
$$

. . .


for some $c_2 \in (c_1,1)$. Here, $c_1$ is from the Armijo condition. The left-hand side is the derivative $\nabla_\alpha \phi(\alpha)$, ensuring that the slope of $\phi(\alpha)$ at the target point is at least $c_2$ times the initial slope $\nabla_\alpha \phi(\alpha)(0)$. Commonly, $c_2 \approx 0.9$ is used for Newton or quasi-Newton methods. Together, the sufficient decrease and curvature conditions form the Wolfe conditions.


:::

::: {.column width="50%"}
![Illustration of curvature condition](Curvature.pdf){width=88%}
:::

::::

## Inexact line search. Wolfe Condition

:::: {.columns}

::: {.column width="50%"}

$$
-\nabla f (x_k - \alpha \nabla f(x_k))^\top \nabla f(x_k) \geq c_2 \nabla f(x_k)^\top(- \nabla f(x_k))
$$

Together, the sufficient decrease and curvature conditions form the Wolfe conditions.

:::

::: {.column width="50%"}

![Illustration of Wolfe condition](Wolfe.pdf){width=88%}
:::

::::


## Backtracking Line Search

Backtracking line search is a technique to find a step size that satisfies the Armijo condition, Goldstein conditions, or other criteria of inexact line search. It begins with a relatively large step size and iteratively scales it down until a condition is met.

. . .


### Algorithm:

1. Choose an initial step size, $\alpha_0$, and parameters $\beta \in (0, 1)$ and $c_1 \in (0, 1)$.
2. Check if the chosen step size satisfies the chosen condition (e.g., Armijo condition).
3. If the condition is satisfied, stop; else, set $\alpha := \beta \alpha$ and repeat step 2.

. . .


The step size $\alpha$ is updated as 

$$
\alpha_{k+1} := \beta \alpha_k
$$

in each iteration until the chosen condition is satisfied.

:::{.callout-example}
In machine learning model training, the backtracking line search can be used to adjust the learning rate. If the loss doesn't decrease sufficiently, the learning rate is reduced multiplicatively until the Armijo condition is met.
:::

## Numerical illustration

![Comparison of different line search algorithms](line_search_comp.pdf)

[Open In Colab $\clubsuit$](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Line_search.ipynb)

# Gradient Descent

## Direction of local steepest descent

:::: {.columns}
::: {.column width="40%"}

Let's consider a linear approximation of the differentiable function $f$ along some direction $h, \|h\|_2 = 1$:

. . .

$$
f(x + \alpha h) = f(x) + \alpha \langle f'(x), h \rangle + o(\alpha)
$$

. . .

We want $h$ to be a decreasing direction:

$$
f(x + \alpha h) < f(x)
$$

$$
f(x) + \alpha \langle f'(x), h \rangle + o(\alpha) < f(x)
$$

. . .

and going to the limit at $\alpha \rightarrow 0$:

$$
\langle f'(x), h \rangle \leq 0
$$

:::

. . .

::: {.column width="60%"}

Also from Cauchy–Bunyakovsky–Schwarz inequality:

$$
\begin{split}
|\langle f'(x), h \rangle | &\leq \| f'(x) \|_2 \| h \|_2 \\
\langle f'(x), h \rangle &\geq -\| f'(x) \|_2 \| h \|_2 = -\| f'(x) \|_2
\end{split}
$$

. . .

Thus, the direction of the antigradient

$$
h = -\dfrac{f'(x)}{\|f'(x)\|_2}
$$

gives the direction of the **steepest local** decreasing of the function $f$.

. . .

The result of this method is

$$
x_{k+1} = x_k - \alpha f'(x_k)
$$

:::
::::

## Convergence of Gradient Descent algorithm

Heavily depends on the choice of the learning rate $\alpha$:

[![](gd_2d.png)](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/GD_2d_visualization.ipynb)

## Exact line search aka steepest descent

:::: {.columns}
::: {.column width="80%"}
$$
\alpha_k = \text{arg}\min_{\alpha \in \mathbb{R^+}} f(x_{k+1}) = \text{arg}\min_{\alpha \in \mathbb{R^+}} f(x_k - \alpha \nabla f(x_k))
$$
More theoretical than practical approach. It also allows you to analyze the convergence, but often exact line search can be difficult if the function calculation takes too long or costs a lot.

An interesting theoretical property of this method is that each following iteration is orthogonal to the previous one:
$$
\alpha_k = \text{arg}\min_{\alpha \in \mathbb{R^+}} f(x_k - \alpha \nabla f(x_k))
$$

. . .

Optimality conditions:

. . .

$$
\nabla f(x_{k+1})^\top \nabla f(x_k) = 0
$$
:::
::: {.column width="20%"}

![Steepest Descent](GD_vs_Steepest.pdf)

[Open In Colab $\clubsuit$](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Steepest_descent.ipynb)
:::
::::

# Strongly convex quadratics

## Coordinate shift

:::: {.columns}

::: {.column width="70%"}

Consider the following quadratic optimization problem:

$$
\label{problem}
\min\limits_{x \in \mathbb{R}^d} f(x) =  \min\limits_{x \in \mathbb{R}^d} \dfrac{1}{2} x^\top  A x - b^\top  x + c, \text{ where }A \in \mathbb{S}^d_{++}.
$$

. . .

* Firstly, without loss of generality we can set $c = 0$, which will or affect optimization process.
* Secondly, we have a spectral decomposition of the matrix $A$: 
    $$
    A = Q \Lambda Q^T
    $$
* Let's show, that we can switch coordinates to make an analysis a little bit easier. Let $\hat{x} = Q^T(x - x^*)$, where $x^*$ is the minimum point of initial function, defined by $Ax^* = b$. At the same time $x = Q\hat{x} + x^*$.
    $$
    \begin{split}
    \uncover<+->{ f(\hat{x}) &= \frac12  (Q\hat{x} + x^*)^\top  A (Q\hat{x} + x^*) - b^\top  (Q\hat{x} + x^*) \\}
    \uncover<+->{ &= \frac12 \hat{x}^T Q^TAQ\hat{x} + (x^*)^TAQ\hat{x} + \frac12 (x^*)^T A (x^*)^T - b^T Q\hat{x} - b^T x^*\\}
    \uncover<+->{ &=  \frac12 \hat{x}^T \Lambda \hat{x}}
    \end{split}
    $$

:::
::: {.column width="30%"}
![](coordinate_shift.pdf)
:::
::::

## Convergence analysis

Now we can work with the function $f(x) = \frac12 x^T \Lambda x$ with $x^* = 0$ without loss of generality (drop the hat from the $\hat{x}$)

:::: {.columns}
::: {.column width="50%"}
$$
\begin{split}
\uncover<+->{x^{k+1} &= x^k - \alpha^k \nabla f(x^k)} 
\uncover<+->{= x^k - \alpha^k \Lambda x^k \\ } 
\uncover<+->{&= (I - \alpha^k \Lambda) x^k \\ }
\uncover<+->{ x^{k+1}_{(i)} &= (1 - \alpha^k \lambda_{(i)}) x^k_{(i)} \text{ For $i$-th coordinate} \\ }
\uncover<+->{  x^{k+1}_{(i)} &= (1 - \alpha^k \lambda_{(i)})^k x^0_{(i)}}
\end{split}
$$
\uncover<+->{
Let's use constant stepsize $\alpha^k = \alpha$. Convergence condition:
$$
\rho(\alpha) = \max_{i} |1 - \alpha \lambda_{(i)}| < 1
$$
Remember, that $\lambda_{\text{min}} = \mu > 0, \lambda_{\text{max}} = L \geq \mu$.}

:::: {.columns}
::: {.column width="50%"}
$$
\begin{split}
\uncover<+->{ |1 - \alpha \mu| &< 1 \\ }
\uncover<+->{ -1 < 1 &- \alpha \mu < 1 \\ }
\uncover<+->{ \alpha < \frac{2}{\mu} \quad & \quad \alpha\mu > 0}
\end{split}
$$
:::
::: {.column width="50%"}
$$
\begin{split}
\uncover<+->{ |1 - \alpha L| &< 1 \\ }
\uncover<+->{ -1 < 1 &- \alpha L < 1 \\ }
\uncover<+->{ \alpha < \frac{2}{L} \quad & \quad \alpha L > 0}
\end{split}
$$
:::
::::

. . .

$\alpha < \frac{2}{L}$ is needed for convergence.

:::

. . .

::: {.column width="50%"}
Now we would like to tune $\alpha$ to choose the best (lowest) convergence rate

$$
\begin{split}
\uncover<+->{ \rho^* &=  \min_{\alpha} \rho(\alpha) } \uncover<+->{  = \min_{\alpha} \max_{i} |1 - \alpha \lambda_{(i)}| \\ }
\uncover<+->{ &=  \min_{\alpha} \left\{|1 - \alpha \mu|, |1 - \alpha L| \right\} \\ }
\uncover<+->{ \alpha^* &: \quad  1 - \alpha^* \mu = \alpha^* L - 1 \\ }
\uncover<+->{ & \alpha^* = \frac{2}{\mu + L} } \uncover<+->{ \quad \rho^* = \frac{L - \mu}{L + \mu} \\ }
\uncover<+->{ x^{k+1} &= \left( \frac{L - \mu}{L + \mu} \right)^k x^0 } \uncover<+->{ \quad f(x^{k+1}) = \left( \frac{L - \mu}{L + \mu} \right)^{2k} f(x^0)}
\end{split}
$$
:::
::::

## Convergence analysis

So, we have a linear convergence in the domain with rate $\frac{\kappa - 1}{\kappa + 1} = 1 - \frac{2}{\kappa + 1}$, where $\kappa = \frac{L}{\mu}$ is sometimes called *condition number* of the quadratic problem.

| $\kappa$ | $\rho$ | Iterations to decrease domain gap $10$ times | Iterations to decrease function gap $10$ times |
|:-:|:-:|:-----------:|:-----------:|
| $1.1$ | $0.05$ | $1$ | $1$ |
| $2$ | $0.33$ | $3$ | $2$ |
| $5$ | $0.67$ | $6$ | $3$ |
| $10$ | $0.82$ | $12$ | $6$ |
| $50$ | $0.96$ | $58$ | $29$ |
| $100$ | $0.98$ | $116$ | $58$ |
| $500$ | $0.996$ | $576$ | $288$ |
| $1000$ | $0.998$ | $1152$ | $576$ |

# Polyak-Lojasiewicz smooth case

## Polyak-Lojasiewicz condition. Linear convergence of gradient descent without convexity

PL inequality holds if the following condition is satisfied for some $\mu > 0$,
$$
\Vert \nabla f(x) \Vert^2 \geq 2 \mu (f(x) - f^*) \quad \forall x
$$
It is interesting, that the Gradient Descent algorithm might converge linearly even without convexity.

The following functions satisfy the PL condition but are not convex. [\faPython Link to the code](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/PL_function.ipynb)

:::: {.columns}

::: {.column width="50%"}

$$
f(x) = x^2 + 3\sin^2(x)
$$

![PL function](pl_2d.pdf){width=65%}

:::

. . .

::: {.column width="50%"}

$$
f(x,y) = \dfrac{(y - \sin x)^2}{2}
$$

![PL function](pl_3d.pdf){width=80%}

:::
::::

## Convergence analysis

:::{.callout-theorem}
Consider the Problem 

$$
f(x) \to \min_{x \in \mathbb{R}^d}
$$

and assume that $f$ is $\mu$-Polyak-Lojasiewicz and $L$-smooth, for some $L\geq \mu >0$.

Consider $(x^k)_{k \in \mathbb{N}}$ a sequence generated by the gradient descent constant stepsize algorithm, with a stepsize satisfying $0<\alpha \leq \frac{1}{L}$. Then:

$$
f(x^{k})-f^* \leq (1-\alpha \mu)^k (f(x^0)-f^*).
$$
:::

# Smooth convex case

## Smooth convex case

:::{.callout-theorem}
Consider the Problem 

$$
f(x) \to \min_{x \in \mathbb{R}^d}
$$

and assume that $f$ is convex and $L$-smooth, for some $L>0$.

Let $(x^{k})_{k \in \mathbb{N}}$ be the sequence of iterates generated by the gradient descent constant stepsize algorithm, with a stepsize satisfying $0 < \alpha\leq \frac{1}{L}$. Then, for all $x^* \in {\rm{argmin}}~f$, for all $k \in \mathbb{N}$ we have that

$$
f(x^{k})-f^* \leq \frac{\|x^0-x^*\| ^2}{2 \alpha k}.
$$
:::


# Lower bounds

## How optimal is $\mathcal{O}\left(\frac1k\right)$?

* Is it somehow possible to understand, that the obtained convergence is the fastest possible with this class of problem and this class of algorithms?
* The iteration of gradient descent:
    $$
    \begin{aligned}
    x^{k+1} &= x^k - \alpha^k \nabla f(x^k)\\
    &= x^{k-1} - \alpha^{k-1} \nabla f(x^{k-1}) - \alpha^k \nabla f(x^k) \\
    & \;\;\vdots \\
    &= x^0 - \sum\limits_{i=0}^k \alpha^{k-i} \nabla f(x^{k-i})
    \end{aligned}
    $$
* Consider a family of first-order methods, where
    $$
    x^{k+1} \in x^0 + \text{span} \left\{\nabla f(x^{0}), \nabla f(x^{1}), \ldots, \nabla f(x^{k})\right\}
    $$ {#eq-fom}

## Smooth convex case

:::{.callout-theorem}
There exists a function $f$ that is $L$-smooth and convex such that any [method @eq-fom] satisfies
$$
\min_{i \in [1, k]} f(x^i) - f^* \geq \frac{3L \|x^0 - x^*\|_2^2}{32(1+k)^2}
$$
:::

. . .

* No matter what gradient method you provide, there is always a function $f$ that, when you apply your gradient method on minimizing such $f$, the convergence rate is lower bounded as $\mathcal{O}\left(\frac{1}{k^2}\right)$.
* The key to the proof is to explicitly build a special function $f$.

# Recap

## Recap

$$
\text{Gradient Descent:} \qquad \qquad \min_{x \in \mathbb{R}^n} f(x) \qquad \qquad x^{k+1} = x^k - \alpha^k \nabla f(x^k)
$$

|convex (non-smooth) | smooth (non-convex) | smooth & convex | smooth & strongly convex (or PL) |
|:-----:|:-----:|:-----:|:--------:|
| $f(x^k) - f^* \sim  \mathcal{O} \left( \dfrac{1}{\sqrt{k}} \right)$ | $\|\nabla f(x^k)\|^2 \sim \mathcal{O} \left( \dfrac{1}{k} \right)$ | $f(x^k) - f^* \sim  \mathcal{O} \left( \dfrac{1}{k} \right)$ | $\|x^k - x^*\|^2 \sim \mathcal{O} \left( \left(1 - \dfrac{\mu}{L}\right)^k \right)$ |
| $k_\varepsilon \sim  \mathcal{O} \left( \dfrac{1}{\varepsilon^2} \right)$ | $k_\varepsilon \sim \mathcal{O} \left( \dfrac{1}{\varepsilon} \right)$ | $k_\varepsilon \sim  \mathcal{O}  \left( \dfrac{1}{\varepsilon} \right)$ | $k_\varepsilon  \sim \mathcal{O} \left( \kappa \log \dfrac{1}{\varepsilon}\right)$ |

. . .

:::: {.columns}

::: {.column width="50%"}
For smooth strongly convex we have:
$$
f(x^{k})-f^* \leq \left(1- \dfrac{\mu}{L}\right)^k (f(x^0)-f^*).
$$
Note also, that for any $x$
$$
1 - x \leq e^{-x}
$$
:::

. . .

::: {.column width="50%"}
Finally we have 
$$
\begin{aligned}
\varepsilon &= f(x^{k_\varepsilon})-f^* \leq  \left(1- \dfrac{\mu}{L}\right)^{k_\varepsilon} (f(x^0)-f^*) \\
&\leq \exp\left(- k_\varepsilon\dfrac{\mu}{L}\right) (f(x^0)-f^*) \\
k_\varepsilon &\geq \kappa \log \dfrac{f(x^0)-f^*}{\varepsilon} = \mathcal{O} \left( \kappa \log \dfrac{1}{\varepsilon}\right)
\end{aligned}
$$
:::

::::

. . .

\uncover<+->{{\bf Question:} Can we do faster, than this using the first-order information? }\uncover<+->{{\bf Yes, we can.}}

# Lower bounds

## Lower bounds 

| convex (non-smooth) | smooth (non-convex)^[[Carmon, Duchi, Hinder, Sidford, 2017](https://arxiv.org/pdf/1710.11606.pdf)] | smooth & convex^[[Nemirovski, Yudin, 1979](https://fmin.xyz/assets/files/nemyud1979.pdf)] | smooth & strongly convex (or PL) |
|:-----:|:-----:|:-----:|:--------:|
| $\mathcal{O} \left( \dfrac{1}{\sqrt{k}} \right)$ | $\mathcal{O} \left( \dfrac{1}{k^2} \right)$ |  $\mathcal{O} \left( \dfrac{1}{k^2} \right)$ | $\mathcal{O} \left( \left(1 - \sqrt{\dfrac{\mu}{L}}\right)^k \right)$ |
| $k_\varepsilon \sim  \mathcal{O} \left( \dfrac{1}{\varepsilon^2} \right)$  | $k_\varepsilon \sim  \mathcal{O}  \left( \dfrac{1}{\sqrt{\varepsilon}} \right)$ | $k_\varepsilon \sim  \mathcal{O}  \left( \dfrac{1}{\sqrt{\varepsilon}} \right)$ | $k_\varepsilon  \sim \mathcal{O} \left( \sqrt{\kappa} \log \dfrac{1}{{\varepsilon}}\right)$ |

## Lower bounds 

:::: {.columns}

::: {.column width="50%"}

The iteration of gradient descent:
$$
\begin{aligned}
x^{k+1} &= x^k - \alpha^k \nabla f(x^k)\\
&= x^{k-1} - \alpha^{k-1} \nabla f(x^{k-1}) - \alpha^k \nabla f(x^k) \\
& \;\;\vdots \\
&= x^0 - \sum\limits_{i=0}^k \alpha^{k-i} \nabla f(x^{k-i})
\end{aligned}
$$

. . .

Consider a family of first-order methods, where
$$
x^{k+1} \in x^0 + \text{span} \left\{\nabla f(x^{0}), \nabla f(x^{1}), \ldots, \nabla f(x^{k})\right\}
$$ {#eq-fom}
:::

. . .

::: {.column width="50%"}

:::{.callout-theorem}

### Non-smooth convex case
There exists a function $f$ that is $M$-Lipschitz and convex such that any first-order method [of the form @eq-fom] satisfies
$$
\min_{i \in [1, k]} f(x^i) - f^* \geq \frac{M \|x^0 - x^*\|_2}{2(1+\sqrt{k})}
$$
:::

. . .

:::{.callout-theorem}

### Smooth and convex case
There exists a function $f$ that is $L$-smooth and convex such that any  first-order method [of the form @eq-fom] satisfies
$$
\min_{i \in [1, k]} f(x^i) - f^* \geq \frac{3L \|x^0 - x^*\|_2^2}{32(1+k)^2}
$$
:::
:::

::::

# Strongly convex quadratic problem

## Oscillations and acceleration

[![](GD_vs_HB_hor.pdf)](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/GD.ipynb)


## Coordinate shift

:::: {.columns}

::: {.column width="70%"}

Consider the following quadratic optimization problem:

$$
\label{problem}
\min\limits_{x \in \mathbb{R}^d} f(x) =  \min\limits_{x \in \mathbb{R}^d} \dfrac{1}{2} x^\top  A x - b^\top  x + c, \text{ where }A \in \mathbb{S}^d_{++}.
$$

. . .

* Firstly, without loss of generality we can set $c = 0$, which will or affect optimization process.
* Secondly, we have a spectral decomposition of the matrix $A$: 
    $$
    A = Q \Lambda Q^T
    $$
* Let's show, that we can switch coordinates in order to make an analysis a little bit easier. Let $\hat{x} = Q^T(x - x^*)$, where $x^*$ is the minimum point of initial function, defined by $Ax^* = b$. At the same time $x = Q\hat{x} + x^*$.
    $$
    \begin{split}
    f(\hat{x}) &= \frac12  (Q\hat{x} + x^*)^\top  A (Q\hat{x} + x^*) - b^\top  (Q\hat{x} + x^*) \\
    &= \frac12 \hat{x}^T Q^TAQ\hat{x} + (x^*)^TAQ\hat{x} + \frac12 (x^*)^T A (x^*)^T - b^T Q\hat{x} - b^T x^*\\
    &=  \frac12 \hat{x}^T \Lambda \hat{x}
    \end{split}
    $$

:::
::: {.column width="30%"}
![](coordinate_shift.pdf)
:::
::::

# Heavy ball

## Polyak Heavy ball method

:::: {.columns}

::: {.column width="25%"}
![](GD_HB.pdf)
:::

::: {.column width="75%"}
Let's introduce the idea of momentum, proposed by Polyak in 1964. Recall that the momentum update is

$$
x^{k+1} = x^k - \alpha \nabla f(x^k) + \beta (x^k - x_{k-1}).
$$

. . .

Which is in our (quadratics) case is
$$
\hat{x}_{k+1} = \hat{x}_k - \alpha \Lambda \hat{x}_k + \beta (\hat{x}_k - \hat{x}_{k-1}) = (I - \alpha \Lambda + \beta I) \hat{x}_k - \beta \hat{x}_{k-1}
$$

. . .

This can be rewritten as follows

$$
\begin{split}
&\hat{x}_{k+1} = (I - \alpha \Lambda + \beta I) \hat{x}_k - \beta \hat{x}_{k-1}, \\
&\hat{x}_{k} = \hat{x}_k.
\end{split}
$$

. . .

Let’s use the following notation $\hat{z}_k = \begin{bmatrix} 
\hat{x}_{k+1} \\
\hat{x}_{k}
\end{bmatrix}$. Therefore $\hat{z}_{k+1} = M \hat{z}_k$, where the iteration matrix $M$ is:

. . .

$$
M = \begin{bmatrix} 
I - \alpha \Lambda + \beta I & - \beta I \\
I & 0_{d}
\end{bmatrix}.
$$

:::
::::

## Reduction to a scalar case

Note, that $M$ is $2d \times 2d$ matrix with 4 block-diagonal matrices of size $d \times d$ inside. It means, that we can rearrange the order of coordinates to make $M$ block-diagonal in the following form. Note that in the equation below, the matrix $M$ denotes the same as in the notation above, except for the described permutation of rows and columns. We use this slight abuse of notation for the sake of clarity. 

. . .

:::: {.columns}

::: {.column width="40%"}

![Illustration of matrix $M$ rearrangement](Rearranging_squares.pdf)

:::
:::{.column width="60%"}
$$
\begin{aligned}
\begin{bmatrix} 
\hat{x}_{k}^{(1)} \\
\vdots \\
\hat{x}_{k}^{(d)} \\
\addlinespace 
\hat{x}_{k-1}^{(1)} \\
\vdots \\
\hat{x}_{k-1}^{(d)}
\end{bmatrix} \to 
\begin{bmatrix} 
\hat{x}_{k}^{(1)} \\
\addlinespace 
\hat{x}_{k-1}^{(1)} \\
\vdots \\
\hat{x}_{k}^{(d)} \\
\addlinespace 
\hat{x}_{k-1}^{(d)}
\end{bmatrix} \quad M = \begin{bmatrix}
M_1\\
&M_2\\
&&\ldots\\
&&&M_d
\end{bmatrix}
\end{aligned}
$$
:::
::::

where $\hat{x}_{k}^{(i)}$ is $i$-th coordinate of vector $\hat{x}_{k} \in \mathbb{R}^d$ and $M_i$ stands for $2 \times 2$ matrix. This rearrangement allows us to study the dynamics of the method independently for each dimension. One may observe, that the asymptotic convergence rate of the $2d$-dimensional vector sequence of $\hat{z}_k$ is defined by the worst convergence rate among its block of coordinates. Thus, it is enough to study the optimization in a one-dimensional case.

## Reduction to a scalar case

For $i$-th coordinate with $\lambda_i$ as an $i$-th eigenvalue of matrix $W$ we have: 

$$
M_i = \begin{bmatrix} 
1 - \alpha \lambda_i + \beta & -\beta \\
1 & 0
\end{bmatrix}.
$$

. . .

The method will be convergent if $\rho(M) < 1$, and the optimal parameters can be computed by optimizing the spectral radius
$$
\alpha^*, \beta^* = \arg \min_{\alpha, \beta} \max_{\lambda \in [\mu, L]} \rho(M) \quad \alpha^* = \dfrac{4}{(\sqrt{L} + \sqrt{\mu})^2}; \quad \beta^* = \left(\dfrac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}\right)^2.
$$

. . .

It can be shown, that for such parameters the matrix $M$ has complex eigenvalues, which forms a conjugate pair, so the distance to the optimum (in this case, $\Vert z_k \Vert$), generally, will not go to zero monotonically. 

## Heavy ball quadratic convergence

We can explicitly calculate the eigenvalues of $M_i$:

$$
\lambda^M_1, \lambda^M_2 = \lambda \left( \begin{bmatrix} 
1 - \alpha \lambda_i + \beta & -\beta \\
1 & 0
\end{bmatrix}\right) = \dfrac{1+\beta - \alpha \lambda_i \pm \sqrt{(1+\beta - \alpha\lambda_i)^2 - 4\beta}}{2}.
$$

. . .

When $\alpha$ and $\beta$ are optimal ($\alpha^*, \beta^*$), the eigenvalues are complex-conjugated pair $(1+\beta - \alpha\lambda_i)^2 - 4\beta \leq 0$, i.e. $\beta \geq (1 - \sqrt{\alpha \lambda_i})^2$.

. . .

$$
\text{Re}(\lambda^M_1) = \dfrac{L + \mu - 2\lambda_i}{(\sqrt{L} + \sqrt{\mu})^2}; \quad \text{Im}(\lambda^M_1) = \dfrac{\pm 2\sqrt{(L - \lambda_i)(\lambda_i - \mu)}}{(\sqrt{L} + \sqrt{\mu})^2}; \quad \vert \lambda^M_1 \vert = \dfrac{L - \mu}{(\sqrt{L} + \sqrt{\mu})^2}.
$$

. . .

And the convergence rate does not depend on the stepsize and equals to $\sqrt{\beta^*}$.

## Heavy Ball quadratics convergence

:::{.callout-theorem}
Assume that $f$ is quadratic $\mu$-strongly convex $L$-smooth quadratics, then Heavy Ball method with parameters
$$
\alpha = \dfrac{4}{(\sqrt{L} + \sqrt{\mu})^2}, \beta = \dfrac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}
$$

converges linearly:

$$
\|x_k - x^*\|_2 \leq \left( \dfrac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1} \right) \|x_0 - x^*\|
$$

:::

## Heavy Ball Global Convergence ^[[Global convergence of the Heavy-ball method for convex optimization, Euhanna Ghadimi et.al.](https://arxiv.org/abs/1412.7457)]

:::{.callout-theorem}
Assume that $f$ is smooth and convex and that

$$
\beta\in[0,1),\quad \alpha\in\biggl(0,\dfrac{2(1-\beta)}{L}\biggr).
$$

Then, the sequence $\{x_k\}$ generated by Heavy-ball iteration satisfies

$$
f(\overline{x}_T)-f^{\star} \leq  \left\{
\begin{array}[l]{ll}
\frac{\Vert x_{0}-x^\star\Vert^2}{2(T+1)}\biggl(\frac{L\beta}{1-\beta}+\frac{1-\beta}{\alpha}\biggr),\;\;\textup{if}\;\;
\alpha\in\bigl(0,\dfrac{1-\beta}{L}\bigr],\\
\frac{\Vert x_{0}-x^\star\Vert^2}{2(T+1)(2(1-\beta)-\alpha L)}\biggl({L\beta}+\frac{(1-\beta)^2}{\alpha}\biggr),\;\;\textup{if}\;\;
\alpha\in\bigl[\dfrac{1-\beta}{L},\dfrac{2(1-\beta)}{L}\bigr),
\end{array}
\right.
$$

where $\overline{x}_T$ is the Cesaro average of the iterates, i.e., 

$$
\overline{x}_T = \frac{1}{T+1}\sum_{k=0}^T x_k.
$$
:::


## Heavy Ball Global Convergence ^[[Global convergence of the Heavy-ball method for convex optimization, Euhanna Ghadimi et.al.](https://arxiv.org/abs/1412.7457)]

:::{.callout-theorem}
Assume that $f$ is smooth and strongly convex and that

$$
\alpha\in(0,\dfrac{2}{L}),\quad 0\leq  \beta<\dfrac{1}{2}\biggl( \dfrac{\mu \alpha}{2}+\sqrt{\dfrac{\mu^2\alpha^2}{4}+4(1-\frac{\alpha L}{2})} \biggr) .
$$

where $\alpha_0\in(0,1/L]$. Then, the sequence $\{x_k\}$ generated by Heavy-ball iteration converges linearly to a unique optimizer $x^\star$. In particular,

$$
f(x_{k})-f^\star \leq q^k (f(x_0)-f^\star),
$$

where $q\in[0,1)$.
:::

## Heavy ball method summary

* Ensures accelerated convergence for strongly convex quadratic problems
* Local accelerated convergence was proved in the original paper.
* Recently was proved, that there is no global accelerated convergence for the method.
* Method was not extremely popular until the ML boom
* Nowadays, it is de-facto standard for practical acceleration of gradient methods, even for the non-convex problems (neural network training)

# Nesterov accelerated gradient

## The concept of Nesterov Accelerated Gradient method

:::: {.columns}

::: {.column width="27%"}
$$
x_{k+1} = x_k - \alpha \nabla f(x_k)
$$
:::
::: {.column width="34%"}
$$
x_{k+1} = x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1})
$$
:::
::: {.column width="39%"}
$$
\begin{cases}y_{k+1} = x_k + \beta (x_k - x_{k-1}) \\ x_{k+1} = y_{k+1} - \alpha \nabla f(y_{k+1}) \end{cases}
$$
:::

::::

. . .

Let's define the following notation

$$
\begin{aligned}
x^+ &= x - \alpha \nabla f(x) \qquad &\text{Gradient step} \\
d_k &= \beta_k (x_k - x_{k-1}) \qquad &\text{Momentum term}
\end{aligned}
$$

Then we can write down:


$$
\begin{aligned}
x_{k+1} &= x_k^+ \qquad &\text{Gradient Descent} \\
x_{k+1} &= x_k^+ + d_k \qquad &\text{Heavy Ball} \\
x_{k+1} &= (x_k + d_k)^+ \qquad &\text{Nesterov accelerated gradient}
\end{aligned}
$$


## NAG convergence for quadratics

## General case convergence

:::{.callout-theorem}
Let $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is convex and $L$-smooth. The Nesterov Accelerated Gradient Descent (NAG) algorithm is designed to solve the minimization problem starting with an initial point $x_0 = y_0 \in \mathbb{R}^n$ and $\lambda_0 = 0$. The algorithm iterates the following steps:
$$
\begin{aligned}
&\textbf{Gradient update: } &y_{k+1} &= x_k - \frac{1}{L} \nabla f(x_k) \\
&\textbf{Extrapolation: } &x_{k+1} &= (1 - \gamma_k)y_{k+1} + \gamma_k y_k \\
&\textbf{Extrapolation weight: } &\lambda_{k+1} &= \frac{1 + \sqrt{1 + 4\lambda_k^2}}{2} \\
&\textbf{Extrapolation weight: } &\gamma_k &= \frac{1 - \lambda_k}{\lambda_{k+1}}
\end{aligned}
$$
The sequences $\{f(y_k)\}_{k\in\mathbb{N}}$ produced by the algorithm will converge to the optimal value $f^*$ at the rate of $\mathcal{O}\left(\frac{1}{k^2}\right)$, specifically:
$$
f(y_k) - f^* \leq \frac{2L \|x_0 - x^*\|^2}{k^2}
$$
:::

## General case convergence

:::{.callout-theorem}
Let $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is $\mu$-strongly convex and $L$-smooth. The Nesterov Accelerated Gradient Descent (NAG) algorithm is designed to solve the minimization problem starting with an initial point $x_0 = y_0 \in \mathbb{R}^n$ and $\lambda_0 = 0$. The algorithm iterates the following steps:
$$
\begin{aligned}
&\textbf{Gradient update: } &y_{k+1} &= x_k - \frac{1}{L} \nabla f(x_k) \\
&\textbf{Extrapolation: } &x_{k+1} &= (1 - \gamma_k)y_{k+1} + \gamma_k y_k \\
&\textbf{Extrapolation weight: } &\gamma_k &= \frac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}
\end{aligned}
$$
The sequences $\{f(y_k)\}_{k\in\mathbb{N}}$ produced by the algorithm will converge to the optimal value $f^*$ linearly:
$$
f(y_k) - f^* \leq \frac{\mu + L}{2}\|x_0 - x^*\|^2_2 \exp \left(-\frac{k}{\sqrt{\kappa}}\right)
$$
:::
